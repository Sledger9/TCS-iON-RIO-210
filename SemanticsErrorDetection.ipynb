{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sledger9/TCS-iON-RIO-210/blob/main/SemanticsErrorDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266e5d5b",
      "metadata": {
        "id": "266e5d5b",
        "outputId": "f7cad949-32a0-4490-ca6f-33b5a33360e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (3.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from pandas->datasets>=1.6.0->happytransformer) (2021.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from pandas->datasets>=1.6.0->happytransformer) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.6.0->happytransformer) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install happytransformer \n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f30f6c",
      "metadata": {
        "id": "81f30f6c",
        "outputId": "2340417c-53e7-4301-c56b-939fa081b6ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Shubhankar\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from datasets import load_dataset\n",
        "from happytransformer import TTSettings\n",
        "from happytransformer import TTTrainArgs\n",
        "from happytransformer import HappyTextToText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29a3b4f",
      "metadata": {
        "id": "c29a3b4f",
        "outputId": "3c2dfb0a-b8fd-461b-ee52-f43a72b9e130"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Shubhankar\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "05/28/2023 17:55:18 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
          ]
        }
      ],
      "source": [
        "happy_tt = HappyTextToText(\"T5\", \"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a9c546",
      "metadata": {
        "id": "a6a9c546"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n",
        "\n",
        "eval_dataset = load_dataset(\"jfleg\", split='test[:]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605e24fb",
      "metadata": {
        "id": "605e24fb",
        "outputId": "eaf6b416-5df3-462a-d012-2bf5ec39d2a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['So I think we would not be alive if our ancestors did not develop sciences and technologies . ', 'So I think we could not live if older people did not develop science and technologies . ', 'So I think we can not live if old people could not find science and technologies and they did not develop . ', 'So I think we can not live if old people can not find the science and technology that has not been developed . ']\n",
            "So I think we would not be alive if our ancestors did not develop sciences and technologies . \n",
            "--------------------------------------------------------\n",
            "['Not for use with a car . ', 'Do not use in the car . ', 'Car not for use . ', 'Can not use the car . ']\n",
            "Not for use with a car . \n",
            "--------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for case in train_dataset[\"corrections\"][:2]:\n",
        "    print(case)\n",
        "    print(case[0])\n",
        "    print(\"--------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d2803e",
      "metadata": {
        "id": "30d2803e"
      },
      "outputs": [],
      "source": [
        "def generate_csv(csv_path, dataset):\n",
        "    with open(csv_path, 'w', newline='') as csvfile:\n",
        "        writter = csv.writer(csvfile)\n",
        "        writter.writerow([\"input\", \"target\"])\n",
        "        for case in dataset:\n",
        "     \t    # Adding the task's prefix to input \n",
        "            input_text = \"grammar: \" + case[\"sentence\"]\n",
        "            for correction in case[\"corrections\"]:\n",
        "                # a few of the cases contain blank strings. \n",
        "                if input_text and correction:\n",
        "                    writter.writerow([input_text, correction])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6577655b",
      "metadata": {
        "id": "6577655b"
      },
      "outputs": [],
      "source": [
        "generate_csv(\"train.csv\", train_dataset)\n",
        "generate_csv(\"eval.csv\", eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19b5d20",
      "metadata": {
        "id": "b19b5d20",
        "outputId": "0a30936e-e627-4d58-dd92-cb303e7d9b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "     -------------------------------------- 219.1/219.1 kB 4.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from accelerate) (1.10.2)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from accelerate) (1.24.3)\n",
            "Requirement already satisfied: psutil in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from accelerate) (5.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\shubhankar\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (4.4.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\shubhankar\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\shubhankar\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\shubhankar\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\shubhankar\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\shubhankar\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\shubhankar\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\shubhankar\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\shubhankar\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\shubhankar\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\shubhankar\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\shubhankar\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\shubhankar\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\shubhankar\\anaconda3\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d8693d",
      "metadata": {
        "id": "b0d8693d",
        "outputId": "70688017-c24b-498c-fc9a-7c9fa42853a8",
        "colab": {
          "referenced_widgets": [
            "293b66312580492dbecc41718ecf7d60",
            "a59dd7a5ddfa479cbd7f3c109c5a6e48",
            "",
            "ded16e256d494963990af8c1073f122c"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "05/28/2023 17:55:38 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to C:/Users/Shubhankar/.cache/huggingface/datasets/csv/default-95b972af66bdce9d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "293b66312580492dbecc41718ecf7d60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a59dd7a5ddfa479cbd7f3c109c5a6e48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating eval split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to C:/Users/Shubhankar/.cache/huggingface/datasets/csv/default-95b972af66bdce9d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ded16e256d494963990af8c1073f122c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2988 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Shubhankar\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3606: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2988' max='2988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2988/2988 05:33]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "before_result = happy_tt.eval(\"eval.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719d513a",
      "metadata": {
        "id": "719d513a",
        "outputId": "24cb6990-836a-439a-e80b-e026a7777d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before loss: 1.2803850173950195\n"
          ]
        }
      ],
      "source": [
        "print(\"Before loss:\", before_result.loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52cced98",
      "metadata": {
        "id": "52cced98",
        "outputId": "83e88abd-d56d-456a-b905-d3851e83de58",
        "colab": {
          "referenced_widgets": [
            "337ad45fd5374fed9903a51e1c70d610",
            "b2c0d6dcfb9546938eacf6a8da09c07c",
            "",
            "c93dbfbe561b4d6aa43f8dcf247cb585"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "05/28/2023 18:13:56 - INFO - happytransformer.happy_transformer -   Preprocessing training data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to C:/Users/Shubhankar/.cache/huggingface/datasets/csv/default-a30dd2e48ab17d8a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "337ad45fd5374fed9903a51e1c70d610",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2c0d6dcfb9546938eacf6a8da09c07c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to C:/Users/Shubhankar/.cache/huggingface/datasets/csv/default-a30dd2e48ab17d8a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c93dbfbe561b4d6aa43f8dcf247cb585",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3016 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "05/28/2023 18:14:01 - INFO - happytransformer.happy_transformer -   Training...\n",
            "C:\\Users\\Shubhankar\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1131' max='1131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1131/1131 1:21:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.582400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.436500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "args = TTTrainArgs(batch_size=8)\n",
        "happy_tt.train(\"train.csv\", args=args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b545796a",
      "metadata": {
        "id": "b545796a",
        "outputId": "7c0978bd-3b9e-401a-bac8-da1eb0f9e92c",
        "colab": {
          "referenced_widgets": [
            "14e96cd0a8334776bbe776108ef2b68a",
            ""
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "05/28/2023 19:35:57 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14e96cd0a8334776bbe776108ef2b68a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2988 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2988' max='2988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2988/2988 06:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After loss:  0.44646382331848145\n"
          ]
        }
      ],
      "source": [
        "before_loss = happy_tt.eval(\"eval.csv\")\n",
        "\n",
        "print(\"After loss: \", before_loss.loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10038b82",
      "metadata": {
        "id": "10038b82"
      },
      "outputs": [],
      "source": [
        "beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495e329e",
      "metadata": {
        "id": "495e329e",
        "outputId": "91bc0975-47ab-4a43-d88c-a0e578329062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This sentence has bad grammar and spelling!\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Example1: \"\"\"\n",
        "example_1 = \"grammar: This sentences, has bads grammar and spelling!\"\n",
        "result_1 = happy_tt.generate_text(example_1, args=beam_settings)\n",
        "print(result_1.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f7d791",
      "metadata": {
        "id": "02f7d791",
        "outputId": "7b166f05-a253-431c-f5b9-c6d4db8c847e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I enjoy writing articles on AI and I also enjoy writing articles on AI .\n"
          ]
        }
      ],
      "source": [
        "example_2 = \"grammar: I am enjoys, writtings articles ons AI and I also enjoyed write articling on AI.\"\n",
        "\n",
        "result_2 = happy_tt.generate_text(example_2, args=beam_settings)\n",
        "print(result_2.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ceda21f",
      "metadata": {
        "id": "7ceda21f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}